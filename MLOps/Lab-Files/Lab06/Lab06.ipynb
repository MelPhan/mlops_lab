{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import scikit-learn libraries for model building and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "        roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "\n",
    "# Explore the first few rows\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "# Feature Engineering (Creating a binary classification for wine quality)\n",
    "df['quality_label'] = df['quality'].apply(lambda x: 1 if x >= 6 else 0)\n",
    "# Drop original 'quality' column\n",
    "df.drop('quality', axis=1, inplace=True)\n",
    "# Preview the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df.drop('quality_label', axis=1)\n",
    "y = df['quality_label']\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "# Train Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "# Predict on test data\n",
    "y_pred_rf = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "# Predict on test data\n",
    "y_pred_gb = gb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_precision = precision_score(y_test, y_pred_rf)\n",
    "rf_recall = recall_score(y_test, y_pred_rf)\n",
    "rf_f1 = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "# Evaluate Gradient Boosting\n",
    "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "gb_precision = precision_score(y_test, y_pred_gb)\n",
    "gb_recall = recall_score(y_test, y_pred_gb)\n",
    "gb_f1 = f1_score(y_test, y_pred_gb)\n",
    "\n",
    "# Print results for both models\n",
    "print(f\"Random Forest - Accuracy: {rf_accuracy:.4f}, Precision: {rf_precision:.4f}, Recall: {rf_recall:.4f}, F1: {rf_f1:.4f}\")\n",
    "print(f\"Gradient Boosting - Accuracy: {gb_accuracy:.4f}, Precision: {gb_precision:.4f}, Recall: {gb_recall:.4f}, F1: {gb_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "# Confusion Matrix for Random Forest\n",
    "rf_cm = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for Gradient Boosting\n",
    "gb_cm = confusion_matrix(y_test, y_pred_gb)\n",
    "sns.heatmap(gb_cm, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('Gradient Boosting Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve for Random Forest\n",
    "rf_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_prob)\n",
    "plt.plot(rf_fpr, rf_tpr, label='Random Forest')\n",
    "\n",
    "# ROC Curve for Gradient Boosting\n",
    "gb_prob = gb_model.predict_proba(X_test)[:, 1]\n",
    "gb_fpr, gb_tpr, _ = roc_curve(y_test, gb_prob)\n",
    "plt.plot(gb_fpr, gb_tpr, label='Gradient Boosting')\n",
    "\n",
    "# Plot ROC Curves\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest\n",
    "rf_feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
    "rf_feature_importance.nlargest(10).plot(kind='barh', title='Random Forest - Top 10 Important Features')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance for Gradient Boosting\n",
    "gb_feature_importance = pd.Series(gb_model.feature_importances_, index=X.columns)\n",
    "gb_feature_importance.nlargest(10).plot(kind='barh', title='Gradient Boosting - Top 10 Important Features', color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve for Random Forest\n",
    "rf_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "rf_precision, rf_recall, _ = precision_recall_curve(y_test, rf_prob)\n",
    "plt.plot(rf_recall, rf_precision, label='Random Forest')\n",
    "\n",
    "# Precision-Recall Curve for Gradient Boosting\n",
    "gb_prob = gb_model.predict_proba(X_test)[:, 1]\n",
    "gb_precision, gb_recall, _ = precision_recall_curve(y_test, gb_prob)\n",
    "plt.plot(gb_recall, gb_precision, label='Gradient Boosting')\n",
    "\n",
    "# Plot Precision-Recall Curves\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-Score vs. Threshold for Random Forest\n",
    "thresholds = np.arange(0.1, 1, 0.1)\n",
    "f1_scores_rf = []\n",
    "f1_scores_gb = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    rf_pred_threshold = (rf_prob >= threshold).astype(int)\n",
    "    gb_pred_threshold = (gb_prob >= threshold).astype(int)\n",
    "            \n",
    "    f1_rf = f1_score(y_test, rf_pred_threshold)\n",
    "    f1_gb = f1_score(y_test, gb_pred_threshold)\n",
    "                        \n",
    "    f1_scores_rf.append(f1_rf)\n",
    "    f1_scores_gb.append(f1_gb)\n",
    "\n",
    "# Plot F1-Score vs Threshold\n",
    "plt.plot(thresholds, f1_scores_rf, label='Random Forest')\n",
    "plt.plot(thresholds, f1_scores_gb, label='Gradient Boosting', color='green')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('F1-Score vs. Decision Threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def plot_cumulative_gains(y_true, y_pred_proba, model_name):\n",
    "    sorted_indices = np.argsort(y_pred_proba)[::-1]\n",
    "    sorted_true = np.array(y_true)[sorted_indices]\n",
    "\n",
    "    cumulative_gains = np.cumsum(sorted_true) / np.sum(sorted_true)\n",
    "    random_line = np.arange(0, 1, 1 / len(y_true))\n",
    "\n",
    "    plt.plot(cumulative_gains, label=f'Cumulative Gains - {model_name}')\n",
    "    plt.plot(random_line, '--', label='Random')\n",
    "    plt.title('Cumulative Gains Chart')\n",
    "    plt.xlabel('Proportion of data examined')\n",
    "    plt.ylabel('Proportion of true positives')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Gain and lift chart for Random Forest\n",
    "plot_cumulative_gains(y_test, rf_prob, 'Random Forest')\n",
    "\n",
    "# Gain and lift chart for Gradient Boosting\n",
    "plot_cumulative_gains(y_test, gb_prob, 'Gradient Boosting')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "rf_metrics = [accuracy_score(y_test, rf_model.predict(X_test)), precision_score(y_test, rf_model.predict(X_test), average='weighted'), recall_score(y_test, rf_model.predict(X_test), average='weighted'), f1_score(y_test, rf_model.predict(X_test), average='weighted')]\n",
    "\n",
    "gb_metrics = [accuracy_score(y_test, gb_model.predict(X_test)), precision_score(y_test, gb_model.predict(X_test), average='weighted'), recall_score(y_test, gb_model.predict(X_test), average='weighted'), f1_score(y_test, gb_model.predict(X_test), average='weighted')]\n",
    "\n",
    "metrics_data = pd.DataFrame({\n",
    "    'Metric': metrics,\n",
    "    'Random Forest': rf_metrics,\n",
    "    'Gradient Boosting': gb_metrics\n",
    "})\n",
    "def spider_plot(metrics_data):\n",
    "    categories = list(metrics_data['Metric'])\n",
    "    N = len(categories)\n",
    "    # Create radar chart for each model\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "    for model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "        values = metrics_data[model_name].tolist()\n",
    "        values += values[:1]  # Closing the plot\n",
    "        angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "        angles += angles[:1]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=model_name)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories)\n",
    "    ax.set_title('Model Performance Comparison', size=20)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "# Create spider plot\n",
    "spider_plot(metrics_data)                                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
